{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. 인공신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 딥러닝의 시작 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 시작 전, 이전에 배운 로지스틱 회귀에 대해서 언급 <br>\n",
    ">> 4장 SGDClassifier : 확률적 경사하강법 <br>\n",
    ">> 표준화 전처리된 데이터를 사용 <br>\n",
    ">> 전처리 이유? : 확률적 경사 하강법은 여러 특성 중 기울기가 가장 가파른 방향을 따라 이동 <br>\n",
    ">> 특성마다 값의 범위가 다르다면.. 올바른 값의 출력이 어려움 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 인공신경망의 등장 <br>\n",
    ">> 이미지 분류에 더 잘맞는다. 이유는 층을 추가하면서 머신의 성능을 높일 수 있기 때문 <br>\n",
    ">> 다만, 가장 기본적인 인공 신경망은 확률적 경사하강법을 사용하는 로지스틱 회귀와 같음<br>\n",
    ">> 입력층 / 출력층 / 뉴런<br>\n",
    "<br>\n",
    ">> ![nn](ch7-1.png) <br>\n",
    "<br>\n",
    ">> 그러나.. 인공 신경망을 만들 때 사용하는 최신 라이브러리들은 SGDClassifier에는 없는 몇 가지 기능을 제공 <br>\n",
    "<br>\n",
    ">>> 라이브러리 : 텐서플로(tensorflow) - made by 구글 <br>\n",
    ">>> 케라스(keras) : 텐서플로의 고수준 API <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 로지스틱 회귀와 인공 신경망의 차이 <br>\n",
    ">> 로지스틱 회귀 : 교차 검증을 사용해 모델을 평가 <br>\n",
    ">> 인공 신경망 : 교차 검증 사용 X, 검증 세트를 별도로 덜어내어 사용 <br>\n",
    "<br>\n",
    ">> 인공 신경망에서 교차 검증을 사용하지 않는 이유 ? <br>\n",
    ">>> 1. 딥러닝 분야의 데이터셋은 충분히 크기 때문에 검증 점수가 안정적 <br>\n",
    ">>> 2. 교차 검증을 수행하기에 훈련 시간이 매우 오래걸림 <br>\n",
    "<br>\n",
    ">>>> 밀집층 : 입력층과 출력층이 빽빽하게 모두 연결되어 있는 것 <br>\n",
    ">>>> 완전 연결층 : 입력층과 출력층의 뉴런이 모두 연결하고 있는 것 <br>\n",
    "<br>\n",
    ">>>> 잠깐 용어정리 <br>\n",
    ">>>> 활성화 함수 : 뉴런의 선형 방정식 계산 결과에 적용되는 함수 <br>\n",
    ">>>> 대표적으로 소프트 맥스 함수 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 케라스 모델 훈련 전 필수사항, 설정 단계에서 손실 함수의 종류 지정<br>\n",
    ">> 이진 분류 : loss='binary_crossentropy' <br>\n",
    ">> 다중 분류 : loss='categorical_crossentropy' <br>\n",
    "<br>\n",
    ">> 케라스에는 아래와 같이 손실함수를 지정하여 사용 <br>\n",
    ">> model.complie(loss='sparse_categorical_crossentropy', metrics='accuracy') <br>\n",
    "<br>\n",
    ">>> 다중 분류에서 사용하는 원-핫 인코딩 : 다중 분류에서 타깃 값을 해당 클래스만 1이고, 나머지는 모두 0인 배열로 만드는 것 <br>\n",
    "<br>\n",
    "> 그런데... 텐서플로에서는 원-핫 인코딩을 사용하지 않아도 됨 <br>\n",
    "> 정수로된 타깃값을 그대로 사용해 크로스 엔트로피 손실을 계산하는 것이 sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 사이킷런 모델과 케라스 모델의 차이 <br>\n",
    ">> 사이킷런 <br>\n",
    ">> 모델 -> sc = SGDClassifier(loss='log', max_iter=5) &nbsp;&nbsp;&nbsp;# log - 손실함수, max_iter - 반복횟수 <br>\n",
    ">> 훈련 -> sc.fit(train_scaled, train_target) <br>\n",
    ">> 평가 -> sc.score(val_scaled, val_target) <br>\n",
    "<br>\n",
    ">> 케라스 <br>\n",
    ">> dense = keras.layers.Dense(10, activation='softmax', input_shape(784,))&nbsp;&nbsp;&nbsp; # Dense - 층 생성 <br>\n",
    ">> 모델 -> model = keras.Sequential(dense)<br>\n",
    ">> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy') <br>\n",
    ">> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # sparse_categorical_crossentropy - 손실함수<br>\n",
    ">> 훈련 -> model.fit(train_scaled, train_target, epochs=5) &nbsp;&nbsp;&nbsp;# epochs - 반복횟수 <br>\n",
    ">> 평가 -> model.evaluate(val_scaled, val_target)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 인공신경망의 장점 : '층'을 더 추가해서 성능을 높일 수 있다. <br>\n",
    "<br>\n",
    "> 은닉층 : 입력층과 출력층 사이에 있는 모든 층 <br>\n",
    "> 은닉층에서는 활성화 함수를 사용 <br>\n",
    "<br>\n",
    "> 출력층에는 아래와 같은 함수를 사용 <br>\n",
    "> 이진분류 - 시그모이드 함수 <br>\n",
    "> 다중분류 - 소프트맥스 함수 <br>\n",
    "<br>\n",
    "> 그런데.. 은닉층에서는 출력층에 비해 시그모이드 함수, 렐루 함수 등 사용이 비교적 자유로움 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 심층 신경망 : 층을 추가하여 입력 데이터에 대한 연속적 학습을 진행해서 성능을 높임<br>\n",
    ">> Sequential 클래스를 사용해서 층을 추가하기 <br>\n",
    ">> 추가된 층 확인하기 : model.summary() <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 렐루 함수<br>\n",
    ">> 등장 이유 : 초기에 시그모이드 함수가 많이 사용되었는데, 이 함수는 왼쪽/오른쪽 끝으로 갈수록 그래프가 누워있어 신속한 대응이 어려움 <br>\n",
    ">> 특히, 층이 많은 심층 신경망일수록 그 효과가 누적되어 올바른 출력이 되지 않음 <br>\n",
    "<br>\n",
    ">> 렐루 함수 : 입력이 양수일 경우, 통과 / 입력이 음수일 경우 0 <br>\n",
    ">>> 렐루 함수에 2차원 값 넣기 위해서는 1차원으로 ... 기존에는 reshape()를 사용해서 변환해주었지만.. <br>\n",
    "<br>\n",
    ">>> 인공신경망에서는 Flatten 클래스 사용 : 배치 차원을 제외하고 나머지 입력 차원을 모두 일렬로 펼치는 역할만 수행 <br>\n",
    ">>>> 입력에 곱해지는 가중치나 절편 X, 인공 신경망의 성능을 위한 기여 X <br>\n",
    ">>>> 입력층과 은닉층 사이에 추가 하기 때문에 '층'이라 부르고, 입력층 바로 뒤에 추가 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 옵티마이저 <br>\n",
    ">> 3장 하이퍼파라미터 : 모델이 학습 X, 사람이 지정해주어야 하는 값 <br>\n",
    ">> 신경망에서의 하이퍼파라미터 : 추가할 은닉층의 수, 뉴런 수, 활성화 함수, 층의 종류, 배치 사이즈, 에포크 등 <br>\n",
    "<br>\n",
    ">> 옵티마이저 : 케라스에서 제공하는 다양한 종류의 알고리즘 <br>\n",
    ">>> 기본 경사 하강법 옵티마이저 : SGD (확률적 경사하강법), 모멘텀, 네스테프로프 모멘텀 등 ... <br>\n",
    ">>> 적응적 학습률 옵티마이저 : RMSprop, Adam, Addgrad 등...\n",
    ">>>> 적응적 학습률이란? : 모델이 최적점에 가까이갈수록 학습률을 낮춤, 모델이 그 이상 훈련되면 오히려 최적점에서 벗어남 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 손실 <br>\n",
    ">> 인공 신경망 모델이 최적화 하는 대상은 정확도가 아니라 손실 함수 <br>\n",
    "<br>\n",
    ">> 파란색 : 훈련 손실 / 주황색 : 검증 손실 <br>\n",
    ">> 다섯번째 에포크 이후 검증 손실이 증가 -> 다섯번째 에포크 이후로 계속 과대 적합이 심해짐 <br>\n",
    "<br>\n",
    "![nn](ch7-2.png)\n",
    "<br>\n",
    ">>> 모델이 잘 훈련되었는지 판단하려면 그동안 '정확도'에 초점을 두었지만 <br>\n",
    ">>> 인공 신경망에서는 손실 함수의 값을 확인하는 것이 더 좋다. 최적화 하는 대상이 손실이기 때문 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 과대 적합을 막기 위한 대표적인 규제 방법 <br>\n",
    "<br>\n",
    "> 드롭아웃 <br>\n",
    ">> 훈련 과정에서 층에 있는 일부 뉴런을 랜덤하게 끼워서 과대 적합을 막음 (또는 일부 뉴런을 0으로 만든다고도 함) <br>\n",
    "<br>\n",
    "![nn](ch7-3.PNG)\n",
    "<br>\n",
    ">> 특정 뉴런에 과대하게 의존하는 것을 막고... 모든 데이터에 대해 주의를 기울여야 하기 때문 <br>\n",
    "<br>\n",
    ">> 그렇지만, 훈련이 끝난 뒤.. 또는 평가/예측 수행 시 드롭아웃을 적용하지 않는다. <br>\n",
    "<br>\n",
    ">> 훈련/평가/예측에는 모든 데이터를 사용해야 올바른 예측 가능하기 때문에. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 최적화된 에포크에서 STOP, 후 최적점을 저장하는 함수 : save_weights() <br>\n",
    "<br>\n",
    "> 콜백 : 훈련 과정 중간에 어떤 작업을 수행할 수 있도록 하는 객체, 클래스 <br>\n",
    "> 예를 들면.. 아래와 같이 훈련 과정에 콜백함수를 사용해서 최적점을 찾아 저장할 수 있음 <br>\n",
    "> checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5', save_best_only=True) <br>\n",
    "<br>\n",
    "> 과대 적합 이전에 끝내기 = 조기종료 <br>\n",
    "> EarlyStopping 콜백 <br>\n",
    "> early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7장 용어 정리\n",
    "\n",
    "> 인공신경망 : 딥러닝 <br>\n",
    "> 텐서플로 : 딥러닝 라이브러리 made by 구글 <br>\n",
    "> 밀집층 : 가장 간단한 인공 신경망의 층 <br>\n",
    "> 원-핫 인코딩 : 정수 값의 배열에서 해당 위치의 원소만 1, 나머지는 0 변환 <br>\n",
    "\n",
    "> 심층 신경망 : 2개 이상의 층을 포함한 신경망 <br>\n",
    "> 렐루 함수 : 이미지 분류 모델의 은닉층에 빈번하게 사용되는 활성화 함수 <br>\n",
    "> 옵티마이저 : 신경망의 가중치와 절편을 핛급하기 위한 알고리즘 또는 방법 <br>\n",
    "\n",
    "> 드롭 아웃 : 은닉층에 있는 뉴런의 출력을 랜덤하게 껴서 과대 적합을 막는 기법 <br>\n",
    "> 콜백 : 케라스 모델을 훈련하는 도중 어떤 작업을 수행하거나 도와주는 도구 <br>\n",
    "> 조기종료 : 검증 점수가 더 이상 감소하지 않고 상승하여 과대적합이 일어나면 훈련 중지 <br>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
