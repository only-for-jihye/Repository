{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ch6. 전이학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 전이학습이란 ?\n",
    "    - 다른 사람이 이미 학습 및 미세조정을 마친 신경망을 내려받아 이 신경망을 출발점으로 삼는 것\n",
    "    - 이미 잘 만들어진 신경망을 활용해 다른 과제를 해결하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 실질적으로 합성곱 신경망을 완전히 처음부터 학습시키는 경우는 드뭄 <br>\n",
    "    1. 데이터 부족\n",
    "        - 신경망을 완전히 처음부터 학습시켜서 어느 정도 성능을 내게 하기 위해서는 대량의 데이터가 필요함\n",
    "        - 대량의 데이터를 수집하는 것은 사실상 어려움\n",
    "        - 데이터도 레이블링 하기 위해서는 쉽지도 않고 비용도 많이 듬\n",
    "    2. 과다한 계산 요구량\n",
    "        - 원하는 문제에 대한 이미지 데이터의 양을 충분히 확보했다 하더라도 이를 신경망에 학습시키기 위해서는 엄청난 양의 계산 자원이 필요함\n",
    "        - 또한 하이퍼파라미터를 조정하기 위해서는 얼마나 오래 걸릴지 예상도 어려움\n",
    "<br><br>\n",
    "* 전이학습의 주요 장점 중 하나 ?\n",
    "    - 일반화 성능을 확보하고 과적합을 방지하는 효과\n",
    "        - 수백만 장의 이미지로 학습된 다른 신경망에서 추출된 특징을 도입함으로써 기존에 없던 새로운 상황에도 더 나은 일반화 성능을 내고 과적합 방지 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 전이학습(transfer learning)의 정의\n",
    ">> 신경망이 어떤 과업을 위해 많은 양의 데이터를 이용해 학습한 지식(특징맵)을 학습 데이터가 상대적으로 적은 다른 유사한 과업으로 옮겨오는 것을 말한다. <br>\n",
    ">> 사전 학습된 신경망의 일부 층을 전용해서 새로운 문제를 해결하기 위해 사용하는 형태다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5장에서 배운 VGGNet, GoogLeNet, ResNet 등의 신경망에 먼저 사전 학습된 가중치를 포함한 모델을 내려받아 '분류기 부분을 제거'하고 우리가 해결하려는 문제의 분류기 부분을 새로 추가하여 만듦\n",
    "    - 사전 학습된 신경망을 특징 추출기로 활용하는 방법이라고 함\n",
    "    <br>![nn](전이학습1.png)<br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 특징기 추출 부분에서 '가중치를 고정시킨다'는 의미\n",
    "    - 미리 학습된 가중치가 추가 학습으로 변하지 않도록 한다는 의미\n",
    "    <br>\n",
    "* 특히, VGGNet을 활용하여 전이학습을 사용할 때의 구성\n",
    "    - VGGNet에서 전용한 특징 추출기는 가중치를 그대로 유지\n",
    "    - 분류기는 아직 학습되지 않은 새로운 소프트맥스 층으로 구성\n",
    "    - 이 모델을 학습할 경우, 새로운 문제를 특정 특징에 따라 잘 분류할 수 있도록 소프트맥스 층만 학습됨\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Page 301~305 까지의 구현된 모델을 통해 알 수 있는 것\n",
    "    1. '학습 불가능한 파라미터'는 대규모 데이터셋을 이미 학습했으므로 가중치를 고정해서 새로운 문제의 특징 추출기로 활용한 것\n",
    "    2. 전이학습을 통해 수백만 장의 이미지를 모두 학습했으므로 성능이 뛰어나다는 것\n",
    "    3. 이런 방법으로 신경망이 이미지에 찍힌 대상의 더 미세한 세부사항까지 놓치지 않고 이해해 새로운 이미지를 대상으로도 더 높은 일반화 성능을 보일 수 있다는 것\n",
    "    4. 즉, 전이학습은 신경망을 학습하는 데에 많은 수고와 비용을 줄여줄 수 있으며 더 나아가 과적합 방지도 해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 전이학습이 어떤 원리로 이 같은 효과를 제공하는가 ?\n",
    "    1. 신경망의 학습 과정에서 실제로 학습되는 것은 무엇인가 ?\n",
    "        - 특징 맵\n",
    "        - 특징 맵은 학습 데이터에 존재하는 특징의 표현\n",
    "    2. 특징은 어떻게 학습되는가 ?\n",
    "        - 역전파 계산 과정에서 가중치는 오차 함숫값이 최소가 되도록 수정되어 최적화된 가중치가 된다.\n",
    "        - 대규모 데이터셋을 대상으로 학습되므로 거의 모든 특징을 이미 추출하여 사용할 수 있는 상태가 됨\n",
    "    3. 특징과 가중치의 관계는 무엇인가 ?\n",
    "        - 특징 맵은 입력 이미지가 가중치 필터를 통과하며 합성곱 연산을 거친 결과다.\n",
    "        - 가중치란 ? 순방향 계산과 역전파 계산을 되풀이하며 반복적으로 수정된 것\n",
    "        - 위의 학습 과정과 하이퍼파라미터 튜닝을 거쳐 신경망이 만족스러운 성능을 보이게 된 상태를 '학습된 신경망'이라 부름\n",
    "    4. 두 신경망 사이에서 실제로 옮겨지는 대상은 무엇일까 ?\n",
    "        - 특징을 옮기기 위해서 사전 학습된 신경망의 최적화된 가중치를 내려받았다. 이들 가중치를 재사용해서 새로운 모델의 시작점으로 삼고 우리가 해결하려는 문제를 위해 다시 학습시킨다.\n",
    "        - 사전 학습된 신경망을 잘 활용하기 위해서는 신경망 구조와 가중치를 함께 내려 받아야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 신경망은 단순한 것부터 각 층마다 차근차근 복잡도를 올려가며 특징을 학습한다. 이 학습된 특징을 '특징 맵'이라 하고\n",
    "* 층을 거쳐가면서 복잡한 특징을 나타내는 '활성화 맵'이 생성된다.\n",
    "<br>\n",
    "* 신경망 뒤쪽 층에서 학습된 특징의 재사용 가능 여부는 기반 모델의 데이터 셋과 새로운 데이터 셋의 유사성과 관계가 깊음\n",
    "    - 모든 이미지에서 모서리나 직선을 찾아볼 수 있으므로 앞쪽 층의 특징은 서로 다른 과업에서도 재사용이 가능함\n",
    "    - 고수준 특징은 과업마다 크게 다름\n",
    "    - 원 도메인과 목표 도메인의 유사성에 따라 고수준 내지 중수준 특징의 재사용 여부를 판단할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 전이학습이 세 가지 방식\n",
    "1. 사전 학습된 신경망을 분류기로 이용하기\n",
    "    - 원 도메인과 목표 도메인이 매우 유사하고 사전 학습된 신경망을 즉시 사용할 수 있는 경우에 적용\n",
    "2. 사전 학습된 신경망을 특징 추출기로 이용하기\n",
    "    - 데이터셋을 학습한 CNN의 특징 추출기 부분 가중치를 고정하고 분류기 부분을 제거한 다음 새로운 분류기 부분을 추가\n",
    "    - 예를 들어 2개의 분류 클래스만 사용하고자 하는데 사전 학습된 신경망이 분류 클래스가 1,000개일 경우 효율적이지 못하기 때문에 분류 클래스만 별도로 생성하여 사용\n",
    "3. 미세 조정하기\n",
    "    - 목표 도메인이 원 도메인과 많이 동떨어진 경우\n",
    "    - 원 도메인에서 정확한 특징을 추출해서 목표 도메인에 맞게 미세조정(fine-tunning)을 거침\n",
    "    - 미세조정이란 ?\n",
    "        - 특징 추출에 쓰이는 신경망의 일부 층을 고정하고 고정하지 않은 층과 새로 추가된 분류기 부분의 층을 함께 학습하는 방식\n",
    "        - 특징 추출기 부분을 재학습하면서 고차원 특징의 표현이 새로운 과업에 적합하게 조정되기 때문\n",
    "    - 사전 학습된 신경망을 새로운 문제에서 재사용하면 한번 최적화된 가중치를 대상으로 학습이 진행됨\n",
    "    - 무작위 값으로 초기화된 가중치와 비교하면 가중치 수렴이 상대적으로 더 빠름\n",
    "    - 전체를 재학습하더라도 사전 학습된 신경망이 처음부터 모델을 학습시키는 것보다는 빠름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 적합한 전이 학습 수준 선택하기\n",
    "    1. 목표 데이터셋의 크기가 작고, 원 도메인과 목표 도메인이 유사하다.\n",
    "        - 사전 학습된 신경망을 특징 추출기로 사용\n",
    "    2. 목표 데이터셋의 크기가 크고, 원 도메인과 목표 도메인이 유사하다.\n",
    "        - 전체 신경망을 미세 조정\n",
    "    3. 목표 데이터셋의 크기가 작고, 원 도메인과 목표 도메인이 크게 다르다.\n",
    "        - 신경망의 앞 부분을 미세 조정\n",
    "    4. 목표 데이터셋의 크기가 크고, 원 도메인과 목표 도메인이 크게 다르다.\n",
    "        - 전체 신경망을 미세 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ch6을 마치며...\n",
    "    1. 전이학습은 분류 또는 물체 인식 프로젝트를 시작하는 출발점으로 유용하다. 특히 학습 데이터를 충분히 확보하지 못했다면 더욱 유용하다.\n",
    "    2. 전이학습은 원 데이터셋에서 학습된 지식을 목표 데이터셋으로 옮기는 방법으로 학습에 소요되는 계산 자원이나 학습 시간을 절약하는 효과가 있다.\n",
    "    3. 신경망은 층수가 깊어질수록 더욱 복잡한 특징을 학습한다. 뒤쪽에 위치한 층일수록 특정 이미지에 가까운 특징이 학습된다.\n",
    "    4. 신경망의 앞쪽 층은 직선, 얼룩, 모서리 등의 저수준 특징을 학습한다. 첫 번째 층의 출력이 두번째 층의 입력이 되며 점차 고수준 특징을 학습한다. 다음 층은 이전 층의 출력을 입력받아 대상의 일부로 조합하고 그 뒤로 이어지는 층이 대상을 탐지한다.\n",
    "    5. 전이학습 방식은 사전 학습된 신경망을 분류기로 사용하는 방식, 특징 추출기로 사용하는 방식, 미세 조정 이렇게 크게 세 가지다.\n",
    "    6. 사전 학습된 신경망을 분류기로 사용하는 방식은 사전 학습된 신경망을 가중치 고정이나 재학습 없이 그대로 사용하는 방식이다.\n",
    "    7. 미세 조정은 특징 추출기 부분 중 일부 층의 가중치를 고정하고, 가중치를 고정하지 않은 층과 새로 추가한 분류기 부분을 함께 재학습 하는 방식이다.\n",
    "    8. 학습된 특징을 다른 신경망으로 옮기는 과정은 성공 가능성은 목표 데이터셋의 규모, 원 도메인과 목표 도메인의 유사성에 따라 결정된다.\n",
    "    9. 일반적으로 미세 조정 중의 재학습에는 신경망 부분별로 학습률을 달리 설정한다. 기존 신경마으이 가중치를 고정하지 안흥ㄴ 부분은 학습률을 작게 설정하고, 새로 추가한 분류기 부분은 학습률을 크게 설정한다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
