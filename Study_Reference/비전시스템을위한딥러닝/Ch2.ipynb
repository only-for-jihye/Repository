{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. 딥러닝과 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 주요 내용 <br>\n",
    "    * 퍼셉트론과 다층 퍼셉트론<br>\n",
    "    * 활성화 함수<br>\n",
    "    * 피드포워드, 오차함수, 오차 최적화를 이용한 신경망 학습<br>\n",
    "    * 역전파 계산과정<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 퍼셉트론 : 뉴런이 1개인 신경망 <br>\n",
    "    * 다층 퍼셉트론 : 뉴런이 다수인 신경망 <br>\n",
    "    * 은닉층 (중간층) : Dense <br>\n",
    "    * 입력벡터, 가중치 벡터, 뉴런함수, 출력\n",
    "        ![nn](Neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 뉴런 함수 <br>\n",
    "    * 가중합 함수 : 선형 결합, 각 가중치를 곱한 입력값의 합에 편향을 더한 값 <br>\n",
    "    * 스텝 함수 : 입력이 0 이상이면 발화, 그렇지 않으면 발화하지 않음. 즉, 입력의 합이 0보다 크면 1, 0보다 작으면 0 (이진 함수) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하나의 뉴런으로 복잡한 문제를 해결할 수 있는가 ? <br>\n",
    "    * 어렵다. 하나의 뉴런은 직선으로 되어 선형 분리 가능 데이터만 가능 <br>\n",
    "    ![nn](선형함수.png)\n",
    "    * 비선형 데이터는 하나의 직선으로 분류하기 어렵다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 복잡한 문제를 해결하기 위해.. 다층 퍼셉트론 <br>\n",
    "    * 은닉층 (Dense) <br>\n",
    "        * 입력층과 출력층 사이에 위치, 특징이 실제로 학습되는 곳, 합성곱 신경망에서 많이 등장 >> '혼공머신'의 7장 심층 신경망 <br>\n",
    "        * 심층 신경망이란 은닉층이 2개 이상인 신경망 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 활성화 함수 (전이함수, 비선형성)\n",
    "    * 가중합의 선형 결합을 비선형 모델로 만듦<br>\n",
    "<br>\n",
    "* 활성화 함수 종류\n",
    "    * 선형 전달 함수 (항등함수, 선형 전달 함수)\n",
    "        * 입력을 그대로 출력하는 함수, 활성화 함수가 없는 효과<br><br>\n",
    "        ![nn](항등함수2.png)\n",
    "    * 헤비사이드 스텝 함수 (스텝 함수, 이진 함수)\n",
    "        * 0과 1만 출력, 입력이 χ > 0이면 발화 (1 출력), 그렇지 않으면 발화 X (0 출력)\n",
    "        * 주로 True or False 등 이진 분류 문제에 사용<br><br>\n",
    "        ![nn](헤비사이드스텝함수2.jpg)\n",
    "    * 시그모이드/로지스틱 함수 (이진 분류)\n",
    "        * 이진 분류에서 두 클래스의 확률을 구할 때 사용\n",
    "        * 모든 입력값을 0 ~ 1 사이의 구간으로 변경, 극단적인 값이나 예욋값을 제거하지 않고 처리 가능\n",
    "        * S-형 커브(S-shape curve)\n",
    "        * 확률을 구하는데 사용 (합격률 or 불합격률)<br><br>\n",
    "        ![nn](시그모이드함수.jpg)\n",
    "    * 소프트맥스 함수 (다중 분류)\n",
    "        * 시그모이드 함수의 일반형\n",
    "        * 3개 이상의 클래스를 대상으로 한 분류에서 각 클래스의 확률을 구할 때 사용\n",
    "        * 출력은 항상 0에서 1 사이, 총합은 항상 1<br><br>\n",
    "        ![nn](소프트맥스함수.png)\n",
    "    * tanh 함수 (하이퍼볼릭 탄젠트 함수)\n",
    "        * 시그모이드 함수를 이동시킨 버전\n",
    "        * -1부터 1사이의 값을 출력\n",
    "        * 시그모이드 함수에 비해 은닉층에서 더 좋은 성능을 보임\n",
    "        * 시그모이드 함수에서는 데이터의 평균이 0.5에 가까워지는 것에 비해, 하이퍼볼릭 탄젠트 함수는 0에 가까워지므로 데이터를 중앙에 모으는 효과가 있음\n",
    "        * 따라서 다음 층의 학습에 유리하기 때문\n",
    "        * 시그모이드 함수와 하이퍼볼릭 탄젠트 함수의 단점으로는 z의 값이 매우 크거나 작을 때, 함수의 기울기가 매우 작아지는 것 (0에 가까움)\n",
    "            ![nn](tanh.png)<br><br>\n",
    "    * ReLU 함수\n",
    "        * 입력이 0보다 크면 그대로, 0보다 작으면 0<br>\n",
    "            ![nn](relu.png)<br><br>\n",
    "    * 누설 ReLU 함수\n",
    "        * ReLU 함수의 단점(x가 음수일 때, 기울기가 0)을 보완\n",
    "        * x < 0 구간에서 함수값이 0이 되지 않도록 작은 기울기 (약 0.01)로 함수값을 음수로 만듦\n",
    "        ![nn](leacky_relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 순방향 계산\n",
    "    * 특징의 선형 결합을 활성화 함수에 통과시키는 계산 과정\n",
    "    * 순방향 계산과 특징 학습은 '혼공 머신'에서 경험해봤으므로 패스\n",
    "    * 간단히 말하면 ?\n",
    "        * 순방향 계산 : 입력층에서 출력층 방향으로 정보가 흘러가기 때문에 붙은 이름\n",
    "            * 가중합과 활성화 함수를 연이어 계산하는 과정을 반복하는 형태로 신경망 각 층을 지나 예측에 이르는 과정\n",
    "        * 특징학습 : 첫번째 층에서 다음 층으로 가면서 특징이 새로운 특징으로 변환되는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 오차 함수 (비용함수, 손실함수)\n",
    "    * 신경망의 예측 결과를 평가하는 것\n",
    "    * 신경망의 예측 결과가 바람직한 출력과 비교해서 얼마나 차이가 있는지 측정하는 수단\n",
    "    * 손실값이 크면 모델의 정확도가 낮다\n",
    "    * 손실값이 작으면 모델의 정확도가 높다\n",
    "    * 손실이 클수록 정확도를 개선하기 위해 모델을 더 많이 학습시켜야 한다\n",
    "    <br><br>\n",
    "    * 최적화 문제\n",
    "        * 오차 함수를 정의하고 파라미터를 조정해서 오차 함수가 계산하는 오차를 최소가 되도록 하는 문제\n",
    "        * 오차 함수 최적화\n",
    "            * 반복마다 무엇을 바꿔야하는지, 오차 함숫값이 최소가 되게 하는 최적의 파라미터를 찾는 것\n",
    "    <br><br>\n",
    "    * 손실 함수\n",
    "        * 평균제곱오차\n",
    "            * 주로 회귀문제에 사용\n",
    "            * 오차를 제곱하기 때문에 오차가 항상 양의 값이며 오찻값이 그대로 결과에 대한 평가가 되므로 계산이 간편함\n",
    "            * 오차를 제곱하기 때문에 예욋값에 민감함\n",
    "            * 예욋값에 대한 민감성 때문에 도움이 될 때가 있지만, 그렇지 않은 경우도 있음\n",
    "                * 이럴 때는 평균보다 중윗값을 중시\n",
    "                    * 절대제곱오차 : 오차의 절댓값의 평균\n",
    "        * 교차 엔트로피\n",
    "            * 주로 분류 문제에 사용\n",
    "            * 두 확률 분포 간의 차이를 측정할 수 있음\n",
    "    <br><br>\n",
    "    * 오차와 가중치의 관계\n",
    "        * 신경망의 학습을 위해서는 오차 함수의 함숫값을 가능한 한 최소의 값으로 되게 해야함\n",
    "        * 즉, 오차가 작을수록 모델이 출력한 예측값의 정확도가 높은 것\n",
    "        * 가중치는 오차를 줄이기 위해 조절하는 신경망의 손잡이 같은 역할<br>\n",
    "        ![nn](079.jpg)<br><br>\n",
    "\n",
    "        * 하강을 따라 오차가 최소가 되는 것이 목표 -> 최적화 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 최적화 알고리즘\n",
    "    * 경사 하강법\n",
    "        * 배치 경사 하강법 (경사 하강법, BGD)\n",
    "            * 가중치를 반복적으로 수정하며 오차 함수의 최저점에 도달할 때까지 오차 함수의 언덕을 내려가는 과정<br>\n",
    "            ![nn](경사하강법.png)<br>\n",
    "            <br>\n",
    "            * 단점 : \n",
    "                1. 지역 극소점 (local minima)가 아니라 전역 최소점 (global minima)로 이동해야 함\n",
    "            <br>![nn](전역최소값.png)<br>\n",
    "                2. 경사를 계산하기 위해 매번 훈련 데이터 전체를 사용해야 함 <br>\n",
    "                    * 즉, 1억 개의 가중치를 1회 수정하기 위해서는 1억 개의 손실값을 합해야 함\n",
    "                    * 훈련 데이터 전체를 하나의 배치로 사용\n",
    "            <br><br><br>\n",
    "        * 확률적 경사 하강법 (SGD)\n",
    "            * 무작위로 데이터 점을 골라 데이터 점 하나를 이용해 가중치를 수정\n",
    "            * 가중치에 다양한 시작점을 만들 수 있고 여러 지역 극소점을 발견할 수 있음\n",
    "            * 여러 지역 극소점 중 가장 작은 값을 전역 최소점으로 인식\n",
    "            <br>![nn](확률적경사하강법.png)<br>\n",
    "            <br><br><br>\n",
    "        * 경사 하강법과 확률적 경사 하강법의 차이\n",
    "            * 경사 하강법\n",
    "                1. 훈련 데이터 전체를 입력한다.\n",
    "                2. 경사를 계산한다.\n",
    "                3. 가중치를 수정한다.\n",
    "                4. n번의 에포크 동안 반복한다.\n",
    "                5. 오차 함수를 따라 하강하는 경로가 매끄럽다.\n",
    "            * 확률적 경사 하강법\n",
    "                1. 훈련 데이터를 무작위로 섞는다.\n",
    "                2. 데이터를 하나 선택해서 입력한다.\n",
    "                3. 경사를 계산한다.\n",
    "                4. 가중치를 수정한다.\n",
    "                5. 또 다른 데이터를 하나 선택해서 입력한다.\n",
    "                6. n번의 에포크 동안 반복한다.\n",
    "                7. 오차 함수를 따라 하강하는 경로가 진동하는 패턴을 보인다.\n",
    "            <br>![nn](차이점.jpg)<br>\n",
    "            * 확률적 경사 하강법은 각각의 가중치 수정이 정확하게 전역 최소점을 향하지 않고, 전역 최소점에 다가가더라도 그 언저리를 맴돌 뿐 정확하게 전역 최소점에 다다르지 못함\n",
    "            <br><br><br>\n",
    "        * 미니배치 경사 하강법 (MB-GD)\n",
    "            * 배치경사 하강법(BGD)과 확률적 경사 하강법(SGD)의 절충안\n",
    "            * 경사를 게산할 때 모든 훈련 데이터(BGD)나 하나의 훈련 데이터(SGD)만 사용하는 대신,\n",
    "            * 몇 개의 미니 배치로 분할한 다음, 이 미니배치로부터 경사를 계산\n",
    "            * 배치경사 하강법(BGD)에 비해 가중치 수정 횟수가 더 많은만큼 더 적은 반복 횟수에서 가중치가 수렴, 벡터 연산을 사용할 수 있어 SGD에 비해 계산 효율도 좋음\n",
    "            * 미니 배치 = 훈련 데이터 수\n",
    "            <br>![nn](미니배치1.jpg)<br>\n",
    "            <br>![nn](미니배치2.jpg)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 역전파 알고리즘\n",
    "    * 지금까지 배운 신경망 학습은 다음 세 단계를 반복하는 과정이었음\n",
    "        1. 순방향 계산\n",
    "        2. 출력값과 정답을 비교해서 오차 함수 또는 손실 함수를 계산\n",
    "        3. 경사 하강법 알고리즘을 사용해 가중치 변화량(Δω)를 계산하고 오차 함숫값을 최적화\n",
    "    <br>\n",
    "    * 역전파 (역방향 계산)\n",
    "        * \"가중치 수정\"을 위해 가중치에 대한 오차의 미분을 출력층부터 첫번째 층까지 전달하는 것\n",
    "        <br>![nn](역전파.jpg)<br>\n",
    "        * 즉, 다시 말하면...\n",
    "        1. 순방향 계산\n",
    "        2. 출력값과 정답을 비교해서 오차 함수 또는 손실 함수를 계산\n",
    "        3. 경사 하강법 알고리즘을 사용해 가중치 변화량(Δω)를 계산하고 오차 함숫값을 최적화\n",
    "        4. <<<역전파를 통해 가중치가 수정됨>>> -> 다시 1부터 시작\n",
    "        <br>\n",
    "        * 이 과정을 반복하면서 오차 함수의 최소점에 다다르게 됨\n",
    "        * 공식과 과정은 어려워서 패스... 개념만 이해했음\n",
    "        <br><br><br>\n",
    "* 역전파 알고리즘에서 기억해야 할 것들\n",
    "    * 역전파는 \"뉴런의 학습이 일어나는 과정\"\n",
    "    * 역전파 과정 중 \"신경망에 포함된 뉴런 간의 연결(가중치)이 손실 함수(신경망의 예측과 정답의 차이)가 최소\"가 되도록 조정\n",
    "    * 가중치가 조정된 결과로 \"은닉층에서 입력층보다 중요한 특징이 나타나게 됨\"\n",
    "    * \"각 층은 자신의 입력 벡터로부터 바람직한 출력 벡터 또는 그와 유사한 벡터를 생성할 수 있는 가중치를 갖는 것\"을 목표로 함, 출력된 값과 원하는 출력갑스이 차이를 \"오차 함수\"라고 함\n",
    "    * 역방향 계산은 신경망의 끝 부분에서 시작해 오차를 역방향으로 전달하고, 연쇄 법칙을 이용해서 지나는 길에 있는 모든 경사를 계산해 마침내 각 가중치를 수정함\n",
    "    * 다시 강조하지만 일반적인 신경망 문제의 목표는 데이터와 가장 잘 부합하는 모델을 발견하는 것, 이는 결국 손실 함숫값이 최소가 되게하는 최선의 가중치를 선택하는 것과 같음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2장 정리\n",
    "    >> 퍼셉트론은 직선(선형 연산)으로 분리 가능한 데이터셋에는 잘 동작한다. <br><br>\n",
    "    >> 직선으로 모델링할 수 없는 비선형 데이터셋에는 뉴런이 더 많이 포함된 신경망이 필요하다.<br><br>\n",
    "    >> 뉴런으로 구성된 층을 모아 다층 퍼셉트론을 만들 수 있다.<br><br>\n",
    "    >> 신경망은 순방향 계산, 오차 계산, 가중치 최적화의 과정을 반복하며 학습한다.<br><br>\n",
    "    >> 파라미터는 가중치나 편이 등 학습 과정에서 신경망에 의해 수정되는 변수를 말한다. 파라미터에 대한 수정은 학습을 통해 자동으로 이루어진다.<br><br>\n",
    "    >> 하이퍼파라미터는 신경망의 층수, 활성화 함수, 손실 함수, 최적화 알고리즘, 조기 종료 여부, 학습률 등 사람이 직접 조정해야 하는 변수다. 이들은 학습을 시작하기 전에 조정을 마쳐야 한다.<br>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
