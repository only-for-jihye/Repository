{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 마치며...ㅋㅋ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본적인 합성곱 신경망 구조는 서로 다른 설정을 가진 합성곱층과 풀링층을 번갈아가며 배치하는 방식으로 구성\n",
    "- LeNet은 5개의 파라미터층으로 구성됨\n",
    "    - 3개는 합성곱층, 2개는 전결합층\n",
    "    - 첫번째와 두번째 합성곱 뒤에는 풀링층이 배치\n",
    "- AlexNet은 LeNet보다 많은 8개의 파라미터 층을 가짐\n",
    "    - 5개는 합성곱층, 3개는 전결합층\n",
    "- VGGNet은 전체 신경망에서 동일한 하이퍼파라미터 설정을 사용하는 방식으로 합성곱층 및 풀링층의 하이퍼파라미터 설정 문제의 해결을 시도함\n",
    "- 인셉션은 VGGNet과는 다른 방식으로 하이퍼파라미터 설정 문제를 해결하려 함\n",
    "    - 특정 필터 또는 풀링 크기를 지정하는 대신 다양한 필터 크기와 풀링 크기를 한꺼번에 사용함\n",
    "- ResNet은 인셉션과 비슷한 접근 방식으로 잔차 블록을 도입, 전체 신경망은 이 잔차 블럭으로 구성\n",
    "    - 신경망의 층수가 일정 이상 늘어나면 기울기 소실 문제로 학습이 잘되지 않는데, 신경망의 서로 떨어진 층을 이어주는 스킵 연결을 도입해 기울기를 전달함\n",
    "    - 이런 방법으로 수백 층에 이르는 거대한 신경망의 학습을 가능케 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요약하면, LeNet -> AlexNet -> VGGNet -> GoogLeNet (inception) -> ResNet로 발전했음"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
